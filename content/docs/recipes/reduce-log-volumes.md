---
weight: 1
---
# Use Dynamic Filtration to Reduce Log Volumes

As they run, services generate log data to provide visibility into what’s happening. Log data can include anything from benign informational messages to full-blown errors. You want to know about errors, but too many informational messages can become costly and distracting noise.

With this recipe, you'll learn how to use MDAI to filter out unnecessary log lines. First, lets set up the log generators and generate log data.

Before you start, make sure that your [dependencies are installed](/docs/installation) and the MDAI Smart Hub is running.

## Start the Loggers

Use the MDAI installation script to deploy three different types of synthetic log generators: normal, noisy, and excessively noisy. The *noisy* logs are the ones that should be dropped.

Run the script from the root directory of the `mdai-labs` repo you cloned.

```
./mdai-kind.sh logs
```

Verify that the log generators are running.

```
kubectl get pods -n mdai -l app.kubernetes.io/part-of=mdai-log-generator
```

You should see output similar to the following.

```
NAME                                  READY   STATUS    RESTARTS   AGE
mdai-logger-d8bb6f897-2q8dn           1/1     Running   0          10m
mdai-logger-d8bb6f897-6pmhl           1/1     Running   0          10m
mdai-logger-d8bb6f897-x8f2j           1/1     Running   0          10m
mdai-logger-noisy-77fcbf8b9f-bfbs4    1/1     Running   0          10m
mdai-logger-noisy-77fcbf8b9f-gh67h    1/1     Running   0          10m
mdai-logger-noisy-77fcbf8b9f-srs6b    1/1     Running   0          10m
mdai-logger-xnoisy-686cb6465f-2pccp   1/1     Running   0          10
mdai-logger-xnoisy-686cb6465f-6kl9c   1/1     Running   0          10m
mdai-logger-xnoisy-686cb6465f-gnjmv   1/1     Running   0          10m
```

Verify that the logs are being generated by running the `kubectl logs` command with any one of the pods.

```
kubectl logs -n mdai {pod name}
```

For example, running the command with one of the noisy log generators should show a variety of log levels.

```
2025-08-29T03:59:10+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:10+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:10+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:10+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:10+00:00 - service1234 - teamA - us-east-1 - WARNING - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - WARNING - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - WARNING - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - WARNING - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-08-29T03:59:11+00:00 - service1234 - teamA - us-east-1 - WARNING - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
```

> [!TIP]
> If you installed K9s, you can use it to observe the logs. To launch the K9s application, enter `k9s` in the terminal window where you launched the cluster (for the correct context). Use the arrow keys to select one of the log generators, then press **l** (lowercase letter l). Press **ESC** to exit the log window. 

## Start a Collector

An OpenTelemetry collector is a component that receives and forwards telemetry data. Start the collector.

```
./mdai-kind.sh collector
```

You can verify that the collector is running with this command.

```
kubectl -n mdai get pods --selector app.kubernetes.io/name=gateway-collector
```

## Start an Observer

An MDAI obserer monitors telemetry data flowing through the cluster's pipelines, performing statistical measurements. It maintains these measurements until Prometheus pulls them. Start the observer.

```
kubectl apply -f ./mdai/observer/0.8.5/observer_ref.yaml -n mdai
```

## Forward the Logs

To forward the logs to the collector, install and deploy Fluentd.

```
./mdai-kind.sh fluentd
```

You can verify that Fluentd is running with this command.

```
kubectl get pods
```

Look at the Fluentd logs, which should indicate that various pod log files are being accessed.

```
kubectl logs svc/fluent-fluentd
```

You should see log lines similar to the following.

```
2025-08-29 17:59:46.781871047 +0000 kubernetes.var.log.containers.mdai-logger-noisy-77fcbf8b9f-gh67h_mdai_mdai-logger-noisy-1b620238a587bd3d74d7df3510885cb6088c084643d0c199d9cb62c1833a699a.log: {"timestamp":"2025-08-29T17:59:46+00:00","mdai_service":"service1234","team":"teamA","region":"us-east-1","level":"ERROR","message":"The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams."}
2025-08-29 17:59:46.781877089 +0000 kubernetes.var.log.containers.mdai-logger-noisy-77fcbf8b9f-gh67h_mdai_mdai-logger-noisy-1b620238a587bd3d74d7df3510885cb6088c084643d0c199d9cb62c1833a699a.log: {"timestamp":"2025-08-29T17:59:46+00:00","mdai_service":"service1234","team":"teamA","region":"us-east-1","level":"WARNING","message":"The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams."}
2025-08-29 17:59:46.781879797 +0000 kubernetes.var.log.containers.mdai-logger-noisy-77fcbf8b9f-gh67h_mdai_mdai-logger-noisy-1b620238a587bd3d74d7df3510885cb6088c084643d0c199d9cb62c1833a699a.log: {"timestamp":"2025-08-29T17:59:46+00:00","mdai_service":"service1234","team":"teamA","region":"us-east-1","level":"INFO","message":"The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams."}
2025-08-29 17:59:46.781882672 +0000 kubernetes.var.log.containers.mdai-logger-noisy-77fcbf8b9f-gh67h_mdai_mdai-logger-noisy-1b620238a587bd3d74d7df3510885cb6088c084643d0c199d9cb62c1833a699a.log: {"timestamp":"2025-08-29T17:59:46+00:00","mdai_service":"service1234","team":"teamA","region":"us-east-1","level":"ERROR","message":"The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams."}
2025-08-29 17:59:46.781887339 +0000 kubernetes.var.log.containers.mdai-logger-noisy-77fcbf8b9f-gh67h_mdai_mdai-logger-noisy-1b620238a587bd3d74d7df3510885cb6088c084643d0c199d9cb62c1833a699a.log: {"timestamp":"2025-08-29T17:59:46+00:00","mdai_service":"service1234","team":"teamA","region":"us-east-1","level":"ERROR","message":"The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams."}
2025-08-29 17:59:46.781894339 +0000 kubernetes.var.log.containers.mdai-logger-noisy-77fcbf8b9f-gh67h_mdai_mdai-logger-noisy-1b620238a587bd3d74d7df3510885cb6088c084643d0c199d9cb62c1833a699a.log: {"timestamp":"2025-08-29T17:59:46+00:00","mdai_service":"service1234","team":"teamA","region":"us-east-1","level":"INFO","message":"The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams."
```

Log data is now flowing through a local pipeline.

## Get Insights

Dashboards are an indispensable devops tool providing alerts, enhancing visibility, and enabling analysis. You'll use dashboards to gain insights into the log data being generated and find ways to optimize the volume of data being transmitted and processed.

### View Log Volumes

[Prometheus](https://prometheus.io/) was installed with the MDAI cluster to collect and aggregate log data from the generators you deployed. Port-forward Prometheus so you can view it in your browser.

```
kubectl port-forward -n mdai svc/kube-prometheus-stack-prometheus 9090:9090
```

Connect to the [Prometheus expression dashboard](http://localhost:9090) and use [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) to query the data flowing through the pipeline.

For example, run this query to see the volume of logs that each of the synthetic services is sending:

```
sum by (mdai_service) (increase(bytes_received_by_service_total[6m]))
```

A graph similar to the following should appear.

![Prometheus dashboard showing Promql query](../promql.png)

Notice that the log volumes generated by services `service4321` and `service1234` are well above the others. To see that more clearly, scroll down to show the list of services below the chart, then click one of the other services (for example, `service1`).

![Prometheus dashboard showing PromQL query filtered to service 1](../promql_service1.png)

Notice that the change of scale on the graph is in orders of magnitude.

### Investigate the Noisy Services

There are 2 services outputting substantially more logs than the others. We'd like to know if that high volume is warranted, and if not, to reduce it. We need to see what kind of logs the 2 noisy services are generating before formulating a plan to dampen their output.

The noisy log generators are responsible for the noisy services, so let’s check one of them. Substitute the name of one of your a noisy loggers in the following command.

```
kubectl logs -n mdai mdai-logger-noisy-77fcbf8b9f-fj2ls
```

The output shows mixed levels for the log lines of `service1234`.

```
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - WARNING - Getting hot in here.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - WARNING - Getting hot in here.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - WARNING - Getting hot in here.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - WARNING - Getting hot in here.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - WARNING - Getting hot in here.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - WARNING - Getting hot in here.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - WARNING - Getting hot in here.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-30T00:36:26+00:00 - service1234 - teamA - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
```


Now let's check one of the extra noisy ones.

```
kubectl logs -n mdai mdai-logger-xnoisy-686cb6465f-lg5gd
```

The output shows that the level of the vast majority of log lines for `service4321` is INFO.

```
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - ERROR - The algorithm failed to execute. Error code 00x00.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
2025-05-29T07:27:52+00:00 - service4321 - teamB - us-east-1 - INFO - The algorithm successfully executed, triggering neural pathways and producing a burst of optimized data streams.
```

We want to know when things are about to go wrong (WARNING), or when they've actually gone wrong (ERROR), but knowing that services are performing as expected (INFO) isn't worth paying for in our use case.

Let's use Grafana to get a consolidated view of the total log volume. Port-forward Grafana so you can view it in your browser.

```
kubectl port-forward -n mdai svc/mdai-grafana 3000:80
```

The [MDAI Grafana dashboards](http://localhost:3000/dashboards) shows summaries of cluster usage, runtime metrics, and more. Log in with the username `admin` and the password `mdai`.

Load the **MDAI Data Management** dashboard and set the time interval to the last hour. You can also use this URL.

```
http://localhost:3000/d/de978rcegwfswb/mdai-data-management?orgId=1&refresh=5s&from=now-1h&to=now
```

Your dashboard should look similar to the following.

![Grafana MDAI Data Management dashboard](../grafana_mdai_data_mgmt.png)

Notice that there are over 50 services operating, and that the log volume recieved and sent is over a gigabyte. Scroll down to the table just below the charts to see a list of services ordered by volume. It's obvious that 2 of the services are by far the noisiest, just as we saw in Prometheus.

### Filter Out the Noise

Infrastructure produces a tremendous volume of log data that then gets monitored, transmitted, analyzed, and stored. The cost to manage logs, in both time and resources, can be significant. We'd like to control that using *MDAI managed filters*. Managed filters are found in the OpenTelemetry collector's configuration file. 

Open the file `../mdai-labs/otel/otel_ref.yaml`, then look for and uncomment this configuration block.

```
      # filter/service_list:
      #   error_mode: ignore
      #   logs:
      #     log_record:
      #       - 'IsMatch(attributes["mdai_service"], "${env:SERVICE_LIST_REGEX}")'
```

This configuration defines an OpentTelemetry processor within the collector. In the `pipelines` configuration block, find and uncomment the line `filter/service_list,`. Save the file, then apply it.

```
kubectl apply -f ./otel/otel_ref.yaml --namespace mdai
```

Over time, you'll see the amount of sent data beging to drop as the percentage different between received and sent data grows.

![Grafana dashboard showing sent data dropping](../grafana_mdai_data_filtered.png)

### How Does Filtering Work?

TODO:
DISCUSS HOW FILTERING WORKS
AND HOW WE'RE FILTERING INDISCREMINITLY, DROPPING ERRORS AND WARNINGS TOO.
GO TO ADVANCED FILTERING TO LEARN HOW TO CREATE A SMARTER FILTER.

## Advanced Filtering Setup

Our original goal was to drop only unneeded log lines from our noisy services. We wanted to keep the WARNINGS and ERRORS because they indicate that a service may need attention. To learn how to create a filter that can do that, see [LINK TO ADVANCED FILTERING].

To try a different recipe, see [Recipe List](../).
